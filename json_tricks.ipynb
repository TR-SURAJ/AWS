{"cells":[{"cell_type":"code","source":["# Write the sample files.\n\n# A regular JSON file.\ndbutils.fs.put(\"/tmp/test1.json\", \n'''{ \"Text1\":\"hello\", \"Text2\":\"goodbye\", \"Num1\":5, \"Array1\":[7,8,9] }\n{ \"Text1\":\"this\", \"Text2\":\"that\", \"Num1\":6.6, \"Array1\":[77,88,99] }\n{ \"Text1\":\"yes\", \"Text2\":\"no\", \"Num1\":-0.03, \"Array1\":[555,444,222] }''', \nTrue)\n\n# A text file that contains one JSON field. \n# We use vertical bar as the field separator because that eliminates problems with quotes and commas inside the JSON.\ndbutils.fs.put(\"/tmp/test2.txt\", \n'''Text1|Text2|Num1|JSON1\nhello | goodbye | 5 | {\"Sub1\":\"john\", \"Sub2\":3}\nthis | that | 6.6 | {\"Sub1\":\"betty\", \"Sub2\":4}\nyes | no | -0.03 | {\"Sub1\":\"bobby\", \"Sub2\":5}''',\nTrue)\n\n# A text file that contains one JSON field that is an array of objects. \ndbutils.fs.put(\"/tmp/test3.txt\", \n'''Text1|Text2|Num1|JSON1\nhello | goodbye | 5 | [{\"Sub1\":\"stop\", \"Sub2\":3}, {\"Sub1\":\"go\", \"Sub2\":6}]\nthis | that | 6.6 | [{\"Sub1\":\"eggs\", \"Sub2\":4}, {\"Sub1\":\"bacon\", \"Sub2\":8}]\nyes | no | -0.03 | [{\"Sub1\":\"apple\", \"Sub2\":5}, {\"Sub1\":\"pear\", \"Sub2\":10}]''',\nTrue)\n\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0ca3291-ebff-4513-a00d-8342c9f587ea"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Wrote 204 bytes.\nWrote 163 bytes.\nWrote 249 bytes.\nOut[1]: True","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Wrote 204 bytes.\nWrote 163 bytes.\nWrote 249 bytes.\nOut[1]: True"]}}],"execution_count":0},{"cell_type":"code","source":["# Verify the presence of the files in the Databricks file system.\n\ndbutils.fs.ls(\"/tmp/\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c52507d-1644-4fc1-bb00-24f5402356b9"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[2]: [FileInfo(path='dbfs:/tmp/hive/', name='hive/', size=0),\n FileInfo(path='dbfs:/tmp/test1.json', name='test1.json', size=204),\n FileInfo(path='dbfs:/tmp/test2.txt', name='test2.txt', size=163),\n FileInfo(path='dbfs:/tmp/test3.txt', name='test3.txt', size=249)]","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[2]: [FileInfo(path='dbfs:/tmp/hive/', name='hive/', size=0),\n FileInfo(path='dbfs:/tmp/test1.json', name='test1.json', size=204),\n FileInfo(path='dbfs:/tmp/test2.txt', name='test2.txt', size=163),\n FileInfo(path='dbfs:/tmp/test3.txt', name='test3.txt', size=249)]"]}}],"execution_count":0},{"cell_type":"code","source":["# Show the file that contains plain JSON.\n\ndbutils.fs.head(\"/tmp/test1.json\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1519617e-6649-44d8-bd41-12d26f333a05"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[3]: '{ \"Text1\":\"hello\", \"Text2\":\"goodbye\", \"Num1\":5, \"Array1\":[7,8,9] }\\n{ \"Text1\":\"this\", \"Text2\":\"that\", \"Num1\":6.6, \"Array1\":[77,88,99] }\\n{ \"Text1\":\"yes\", \"Text2\":\"no\", \"Num1\":-0.03, \"Array1\":[555,444,222] }'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[3]: '{ \"Text1\":\"hello\", \"Text2\":\"goodbye\", \"Num1\":5, \"Array1\":[7,8,9] }\\n{ \"Text1\":\"this\", \"Text2\":\"that\", \"Num1\":6.6, \"Array1\":[77,88,99] }\\n{ \"Text1\":\"yes\", \"Text2\":\"no\", \"Num1\":-0.03, \"Array1\":[555,444,222] }'"]}}],"execution_count":0},{"cell_type":"code","source":["#Make a DataFrame from this file. PySpark will infer the data types of the JSON elements.\n\ntest1DF = spark.read.json(\"/tmp/test1.json\")\n\n# Show the DataFrame. Note that the data is no longer in JSON format. All the JSON syntax has been stripped out and we have regular data fields.\n\ndisplay (test1DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dd197608-ac4f-4338-b09d-28bb18f425b2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[[[7,8,9],5.0,"hello","goodbye"],[[77,88,99],6.6,"this","that"],[[555,444,222],-0.03,"yes","no"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Array1","type":"{\"type\":\"array\",\"elementType\":\"long\",\"containsNull\":true}","metadata":"{}"},{"name":"Num1","type":"\"double\"","metadata":"{}"},{"name":"Text1","type":"\"string\"","metadata":"{}"},{"name":"Text2","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Array1</th><th>Num1</th><th>Text1</th><th>Text2</th></tr></thead><tbody><tr><td>List(7, 8, 9)</td><td>5.0</td><td>hello</td><td>goodbye</td></tr><tr><td>List(77, 88, 99)</td><td>6.6</td><td>this</td><td>that</td></tr><tr><td>List(555, 444, 222)</td><td>-0.03</td><td>yes</td><td>no</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Show the file with regular fields and one field with JSON text.\n\ndbutils.fs.head(\"/tmp/test2.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3fcfc682-0774-4225-b472-1e4593f36df2"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Out[5]: 'Text1|Text2|Num1|JSON1\\nhello | goodbye | 5 | {\"Sub1\":\"john\", \"Sub2\":3}\\nthis | that | 6.6 | {\"Sub1\":\"betty\", \"Sub2\":4}\\nyes | no | -0.03 | {\"Sub1\":\"bobby\", \"Sub2\":5}'","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Out[5]: 'Text1|Text2|Num1|JSON1\\nhello | goodbye | 5 | {\"Sub1\":\"john\", \"Sub2\":3}\\nthis | that | 6.6 | {\"Sub1\":\"betty\", \"Sub2\":4}\\nyes | no | -0.03 | {\"Sub1\":\"bobby\", \"Sub2\":5}'"]}}],"execution_count":0},{"cell_type":"code","source":["# Make a DataFrame from the text file that contains a JSON field.\n\n# This is a small file, so we can infer the schema. For a large file, you should use .schema().\ntest2DF = spark.read.option(\"inferSchema\", True).option(\"header\", True).option(\"delimiter\", \"|\").csv(\"/tmp/test2.txt\")\n\n# Show the DataFrame. Note that the JSON field is plain text; we cannot pick out its sub-fields.\ndisplay (test2DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"744c305b-c238-4fc4-a0ae-c8eee57f6335"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["hello "," goodbye ",5.0," {\"Sub1\":\"john\", \"Sub2\":3}"],["this "," that ",6.6," {\"Sub1\":\"betty\", \"Sub2\":4}"],["yes "," no ",-0.03," {\"Sub1\":\"bobby\", \"Sub2\":5}"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Text1","type":"\"string\"","metadata":"{}"},{"name":"Text2","type":"\"string\"","metadata":"{}"},{"name":"Num1","type":"\"double\"","metadata":"{}"},{"name":"JSON1","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Text1</th><th>Text2</th><th>Num1</th><th>JSON1</th></tr></thead><tbody><tr><td>hello </td><td> goodbye </td><td>5.0</td><td> {\"Sub1\":\"john\", \"Sub2\":3}</td></tr><tr><td>this </td><td> that </td><td>6.6</td><td> {\"Sub1\":\"betty\", \"Sub2\":4}</td></tr><tr><td>yes </td><td> no </td><td>-0.03</td><td> {\"Sub1\":\"bobby\", \"Sub2\":5}</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Change the JSON string into a struct, so we can get its parts. Then pull out one of those parts.\n\nfrom pyspark.sql.functions import from_json, col\nfrom pyspark.sql.types import StructType, StructField, StringType, IntegerType\n\n# Define the schema of the JSON string.\nschema = StructType([\n  StructField(\"Sub1\", StringType()), \n  StructField(\"Sub2\", IntegerType()\n  )\n])\n\n# Use the schema to change the JSON string into a struct, overwriting the JSON string.\ntest2DF = test2DF.withColumn(\"JSON1\", from_json(col(\"JSON1\"), schema))\n\n# Make a separate column from one of the struct fields.\ntest2DF = test2DF.withColumn(\"JSON1_Sub2\", col(\"JSON1.Sub2\"))\n\ndisplay (test2DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"acc17aa2-1fe1-49a1-aa9a-6795336c139d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"test2DF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Text1","nullable":true,"type":"string"},{"metadata":{},"name":"Text2","nullable":true,"type":"string"},{"metadata":{},"name":"Num1","nullable":true,"type":"double"},{"metadata":{},"name":"JSON1","nullable":true,"type":{"fields":[{"metadata":{},"name":"Sub1","nullable":true,"type":"string"},{"metadata":{},"name":"Sub2","nullable":true,"type":"integer"}],"type":"struct"}},{"metadata":{},"name":"JSON1_Sub2","nullable":true,"type":"integer"}],"type":"struct"},"tableIdentifier":null}],"data":[["hello "," goodbye ",5.0,["john",3],3],["this "," that ",6.6,["betty",4],4],["yes "," no ",-0.03,["bobby",5],5]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Text1","type":"\"string\"","metadata":"{}"},{"name":"Text2","type":"\"string\"","metadata":"{}"},{"name":"Num1","type":"\"double\"","metadata":"{}"},{"name":"JSON1","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"Sub1\",\"type\":\"string\",\"nullable\":true,\"metadata\":{}},{\"name\":\"Sub2\",\"type\":\"integer\",\"nullable\":true,\"metadata\":{}}]}","metadata":"{}"},{"name":"JSON1_Sub2","type":"\"integer\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Text1</th><th>Text2</th><th>Num1</th><th>JSON1</th><th>JSON1_Sub2</th></tr></thead><tbody><tr><td>hello </td><td> goodbye </td><td>5.0</td><td>List(john, 3)</td><td>3</td></tr><tr><td>this </td><td> that </td><td>6.6</td><td>List(betty, 4)</td><td>4</td></tr><tr><td>yes </td><td> no </td><td>-0.03</td><td>List(bobby, 5)</td><td>5</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Show the file that contains an array of JSON objects.\n\ndbutils.fs.head(\"/tmp/test3.txt\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2a6f8f4a-ff7f-438e-87df-c3708e7eeb1c"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">Out[10]: &#39;Text1|Text2|Num1|JSON1\\nhello | goodbye | 5 | [{&#34;Sub1&#34;:&#34;stop&#34;, &#34;Sub2&#34;:3}, {&#34;Sub1&#34;:&#34;go&#34;, &#34;Sub2&#34;:6}]\\nthis | that | 6.6 | [{&#34;Sub1&#34;:&#34;eggs&#34;, &#34;Sub2&#34;:4}, {&#34;Sub1&#34;:&#34;bacon&#34;, &#34;Sub2&#34;:8}]\\nyes | no | -0.03 | [{&#34;Sub1&#34;:&#34;apple&#34;, &#34;Sub2&#34;:5}, {&#34;Sub1&#34;:&#34;pear&#34;, &#34;Sub2&#34;:10}]&#39;</div>","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">Out[10]: &#39;Text1|Text2|Num1|JSON1\\nhello | goodbye | 5 | [{&#34;Sub1&#34;:&#34;stop&#34;, &#34;Sub2&#34;:3}, {&#34;Sub1&#34;:&#34;go&#34;, &#34;Sub2&#34;:6}]\\nthis | that | 6.6 | [{&#34;Sub1&#34;:&#34;eggs&#34;, &#34;Sub2&#34;:4}, {&#34;Sub1&#34;:&#34;bacon&#34;, &#34;Sub2&#34;:8}]\\nyes | no | -0.03 | [{&#34;Sub1&#34;:&#34;apple&#34;, &#34;Sub2&#34;:5}, {&#34;Sub1&#34;:&#34;pear&#34;, &#34;Sub2&#34;:10}]&#39;</div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Make a DataFrame from the text file that contains a JSON array.\n\ntest3DF = spark.read.option(\"inferSchema\", True).option(\"header\", True).option(\"delimiter\", \"|\").csv(\"/tmp/test3.txt\")\n\ndisplay (test3DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"afc6d2bb-7b09-4c48-8ed1-9d7fd2713a5b"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"test3DF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Text1","nullable":true,"type":"string"},{"metadata":{},"name":"Text2","nullable":true,"type":"string"},{"metadata":{},"name":"Num1","nullable":true,"type":"double"},{"metadata":{},"name":"JSON1","nullable":true,"type":"string"}],"type":"struct"},"tableIdentifier":null}],"data":[["hello "," goodbye ",5.0," [{\"Sub1\":\"stop\", \"Sub2\":3}, {\"Sub1\":\"go\", \"Sub2\":6}]"],["this "," that ",6.6," [{\"Sub1\":\"eggs\", \"Sub2\":4}, {\"Sub1\":\"bacon\", \"Sub2\":8}]"],["yes "," no ",-0.03," [{\"Sub1\":\"apple\", \"Sub2\":5}, {\"Sub1\":\"pear\", \"Sub2\":10}]"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Text1","type":"\"string\"","metadata":"{}"},{"name":"Text2","type":"\"string\"","metadata":"{}"},{"name":"Num1","type":"\"double\"","metadata":"{}"},{"name":"JSON1","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Text1</th><th>Text2</th><th>Num1</th><th>JSON1</th></tr></thead><tbody><tr><td>hello </td><td> goodbye </td><td>5.0</td><td> [{\"Sub1\":\"stop\", \"Sub2\":3}, {\"Sub1\":\"go\", \"Sub2\":6}]</td></tr><tr><td>this </td><td> that </td><td>6.6</td><td> [{\"Sub1\":\"eggs\", \"Sub2\":4}, {\"Sub1\":\"bacon\", \"Sub2\":8}]</td></tr><tr><td>yes </td><td> no </td><td>-0.03</td><td> [{\"Sub1\":\"apple\", \"Sub2\":5}, {\"Sub1\":\"pear\", \"Sub2\":10}]</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Change the JSON field to a true array.\n\n# Credit to https://kontext.tech/column/spark/284/pyspark-convert-json-string-column-to-array-of-object-structtype-in-data-frame\n\nfrom pyspark.sql.functions import from_json, col, split, udf\nfrom pyspark.sql.types import *\nimport json\n\n# Schema for the array of JSON objects.\njson_array_schema = ArrayType(\n  StructType([\n    StructField('Sub1', StringType(), nullable=False), \n    StructField('Sub2', IntegerType(), nullable=False)\n  ])\n)\n\n# Create function to parse JSON using standard Python json library.\ndef parse_json(array_str):\n  json_obj = json.loads(array_str)\n  for item in json_obj:\n    yield (item['Sub1'], item['Sub2'])\n\n# Create a UDF, whose return type of the JSON schema defined above.    \nparse_json_udf = udf(lambda str: parse_json(str), json_array_schema)\n  \n# Use the UDF to change the JSON string into a true array of objects.\ntest3DF = test3DF.withColumn(\"JSON1arr\", parse_json_udf((col(\"JSON1\"))))\n\n# We don't need to JSON text anymore.\ntest3DF = test3DF.drop(\"JSON1\")\n\n# We now have an array of structs with named members.\ndisplay (test3DF)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6116496a-76aa-4303-9f2a-077ef8404f87"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"test3DF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Text1","nullable":true,"type":"string"},{"metadata":{},"name":"Text2","nullable":true,"type":"string"},{"metadata":{},"name":"Num1","nullable":true,"type":"double"},{"metadata":{},"name":"JSON1arr","nullable":true,"type":{"containsNull":true,"elementType":{"fields":[{"metadata":{},"name":"Sub1","nullable":false,"type":"string"},{"metadata":{},"name":"Sub2","nullable":false,"type":"integer"}],"type":"struct"},"type":"array"}}],"type":"struct"},"tableIdentifier":null}],"data":[["hello "," goodbye ",5.0,[["stop",3],["go",6]]],["this "," that ",6.6,[["eggs",4],["bacon",8]]],["yes "," no ",-0.03,[["apple",5],["pear",10]]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Text1","type":"\"string\"","metadata":"{}"},{"name":"Text2","type":"\"string\"","metadata":"{}"},{"name":"Num1","type":"\"double\"","metadata":"{}"},{"name":"JSON1arr","type":"{\"type\":\"array\",\"elementType\":{\"type\":\"struct\",\"fields\":[{\"name\":\"Sub1\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"Sub2\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}}]},\"containsNull\":true}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Text1</th><th>Text2</th><th>Num1</th><th>JSON1arr</th></tr></thead><tbody><tr><td>hello </td><td> goodbye </td><td>5.0</td><td>List(List(stop, 3), List(go, 6))</td></tr><tr><td>this </td><td> that </td><td>6.6</td><td>List(List(eggs, 4), List(bacon, 8))</td></tr><tr><td>yes </td><td> no </td><td>-0.03</td><td>List(List(apple, 5), List(pear, 10))</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# The array of structs might be useful as is. But it is sometimes easier to put each array element in its own DataFrame row.\n\nfrom pyspark.sql.functions import col, explode\n\ntest3DF = test3DF.withColumn(\"JSON1obj\", explode(col(\"JSON1arr\")))\n\n# The column with the array is now redundant.\ntest3DF = test3DF.drop(\"JSON1arr\")\n\ndisplay (test3DF)\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ce6ac555-b8b4-435b-afcd-4cbe1172bf72"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[{"name":"test3DF","typeStr":"pyspark.sql.dataframe.DataFrame","schema":{"fields":[{"metadata":{},"name":"Text1","nullable":true,"type":"string"},{"metadata":{},"name":"Text2","nullable":true,"type":"string"},{"metadata":{},"name":"Num1","nullable":true,"type":"double"},{"metadata":{},"name":"JSON1obj","nullable":true,"type":{"fields":[{"metadata":{},"name":"Sub1","nullable":false,"type":"string"},{"metadata":{},"name":"Sub2","nullable":false,"type":"integer"}],"type":"struct"}}],"type":"struct"},"tableIdentifier":null}],"data":[["hello "," goodbye ",5.0,["stop",3]],["hello "," goodbye ",5.0,["go",6]],["this "," that ",6.6,["eggs",4]],["this "," that ",6.6,["bacon",8]],["yes "," no ",-0.03,["apple",5]],["yes "," no ",-0.03,["pear",10]]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Text1","type":"\"string\"","metadata":"{}"},{"name":"Text2","type":"\"string\"","metadata":"{}"},{"name":"Num1","type":"\"double\"","metadata":"{}"},{"name":"JSON1obj","type":"{\"type\":\"struct\",\"fields\":[{\"name\":\"Sub1\",\"type\":\"string\",\"nullable\":false,\"metadata\":{}},{\"name\":\"Sub2\",\"type\":\"integer\",\"nullable\":false,\"metadata\":{}}]}","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Text1</th><th>Text2</th><th>Num1</th><th>JSON1obj</th></tr></thead><tbody><tr><td>hello </td><td> goodbye </td><td>5.0</td><td>List(stop, 3)</td></tr><tr><td>hello </td><td> goodbye </td><td>5.0</td><td>List(go, 6)</td></tr><tr><td>this </td><td> that </td><td>6.6</td><td>List(eggs, 4)</td></tr><tr><td>this </td><td> that </td><td>6.6</td><td>List(bacon, 8)</td></tr><tr><td>yes </td><td> no </td><td>-0.03</td><td>List(apple, 5)</td></tr><tr><td>yes </td><td> no </td><td>-0.03</td><td>List(pear, 10)</td></tr></tbody></table></div>"]}}],"execution_count":0},{"cell_type":"code","source":["# Get a specific named element from all the JSON objects. This is much easier now that we exploded the array.\n\ndisplay (test3DF.select(\"JSON1obj.Sub1\"))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cb6c2aa3-2f9b-4e47-ab56-cc8e61fcd370"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"overflow":false,"datasetInfos":[],"data":[["stop"],["go"],["eggs"],["bacon"],["apple"],["pear"]],"plotOptions":{"displayType":"table","customPlotOptions":{},"pivotColumns":null,"pivotAggregation":null,"xColumns":null,"yColumns":null},"columnCustomDisplayInfos":{},"aggType":"","isJsonSchema":true,"removedWidgets":[],"aggSchema":[],"schema":[{"name":"Sub1","type":"\"string\"","metadata":"{}"}],"aggError":"","aggData":[],"addedWidgets":{},"metadata":{},"dbfsResultPath":null,"type":"table","aggOverflow":false,"aggSeriesLimitReached":false,"arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .table-result-container {\n    max-height: 300px;\n    overflow: auto;\n  }\n  table, th, td {\n    border: 1px solid black;\n    border-collapse: collapse;\n  }\n  th, td {\n    padding: 5px;\n  }\n  th {\n    text-align: left;\n  }\n</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>Sub1</th></tr></thead><tbody><tr><td>stop</td></tr><tr><td>go</td></tr><tr><td>eggs</td></tr><tr><td>bacon</td></tr><tr><td>apple</td></tr><tr><td>pear</td></tr></tbody></table></div>"]}}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"json_tricks","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":3016130519807609}},"nbformat":4,"nbformat_minor":0}
